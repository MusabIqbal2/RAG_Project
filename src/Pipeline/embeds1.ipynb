{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "file_path = 'D:/IBA/8th Semester/Musab_Bilal_RAG/src/Pipeline/prog-ann.txt'\n",
    "# Read the content of the original file\n",
    "with open(file_path, 'r', encoding='utf-8', errors='replace') as file:\n",
    "    content = file.read()\n",
    "\n",
    "# Replace all tab characters with spaces\n",
    "content = content.replace('\\t', ' ')\n",
    "\n",
    "# Shorten multiple spaces to a single space\n",
    "content = re.sub(' +', ' ', content)\n",
    "\n",
    "# Define the path for the new file\n",
    "new_file_path = 'D:/IBA/8th Semester/Musab_Bilal_RAG/src/Pipeline/prog-llamaparse_spaces.txt'\n",
    "\n",
    "# Write the modified content to the new file\n",
    "with open(new_file_path, 'w', encoding='utf-8') as new_file:\n",
    "    new_file.write(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "336 chunks generated.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import tiktoken\n",
    "\n",
    "def load_text(file_path):\n",
    "    \"\"\"Loads text from a .txt file and normalizes whitespace.\"\"\"\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        text = f.read()\n",
    "    \n",
    "    text = re.sub(r\"\\n{3,}\", \"\\n\\n\", text.strip())  # Normalize excessive newlines\n",
    "    text = re.sub(r\" +\", \" \", text)  # Replace multiple spaces with a single space\n",
    "    return text\n",
    "\n",
    "def split_into_sentences(text):\n",
    "    \"\"\"Splits text into sentences while keeping boundaries intact.\"\"\"\n",
    "    return re.split(r\"(?<=[.!?])\\s+\", text)\n",
    "\n",
    "def merge_small_chunks(chunks, min_tokens, tokenizer):\n",
    "    \"\"\"Merges small chunks intelligently while maintaining coherence.\"\"\"\n",
    "    merged_chunks = []\n",
    "    current_chunk = \"\"\n",
    "\n",
    "    for chunk in chunks:\n",
    "        token_count = len(tokenizer.encode(current_chunk + \" \" + chunk))\n",
    "\n",
    "        if token_count < min_tokens and current_chunk:\n",
    "            current_chunk += \" \" + chunk\n",
    "        else:\n",
    "            if current_chunk:\n",
    "                merged_chunks.append(current_chunk.strip())\n",
    "            current_chunk = chunk\n",
    "\n",
    "    if current_chunk:\n",
    "        merged_chunks.append(current_chunk.strip())\n",
    "\n",
    "    return merged_chunks\n",
    "\n",
    "def enforce_max_chunk_size(chunks, max_tokens, tokenizer, overlap_ratio=0.15):\n",
    "    \"\"\"Ensures chunks do not exceed max token size while preserving coherence.\"\"\"\n",
    "    final_chunks = []\n",
    "\n",
    "    for chunk in chunks:\n",
    "        tokens = tokenizer.encode(chunk)\n",
    "\n",
    "        if len(tokens) <= max_tokens:\n",
    "            final_chunks.append(chunk)\n",
    "        else:\n",
    "            sentences = split_into_sentences(chunk)\n",
    "            temp_chunk = \"\"\n",
    "\n",
    "            for sentence in sentences:\n",
    "                temp_chunk_tokens = len(tokenizer.encode(temp_chunk + \" \" + sentence))\n",
    "\n",
    "                if temp_chunk_tokens <= max_tokens:\n",
    "                    temp_chunk += \" \" + sentence\n",
    "                else:\n",
    "                    final_chunks.append(temp_chunk.strip())\n",
    "                    overlap_tokens = int(len(tokenizer.encode(temp_chunk)) * overlap_ratio)\n",
    "                    temp_chunk = tokenizer.decode(tokenizer.encode(temp_chunk)[-overlap_tokens:]) + \" \" + sentence\n",
    "\n",
    "            if temp_chunk:\n",
    "                final_chunks.append(temp_chunk.strip())\n",
    "\n",
    "    return final_chunks\n",
    "\n",
    "def process_text(file_path, min_tokens=200, max_tokens=250, overlap_ratio=0.15):\n",
    "    \"\"\"Processes text into optimized chunks based on token constraints.\"\"\"\n",
    "    text = load_text(file_path)\n",
    "    tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "    sentences = split_into_sentences(text)\n",
    "    merged_chunks = merge_small_chunks(sentences, min_tokens, tokenizer)\n",
    "    final_chunks = enforce_max_chunk_size(merged_chunks, max_tokens, tokenizer, overlap_ratio)\n",
    "    \n",
    "    return final_chunks\n",
    "\n",
    "# Usage\n",
    "file_path = 'D:/IBA/8th Semester/Musab_Bilal_RAG/src/Pipeline/prog-ann.txt'\n",
    "chunks = process_text(file_path)\n",
    "print(len(chunks), \"chunks generated.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk size: 1506 characters, 443 tokens\n",
      "Chunk size: 1008 characters, 193 tokens\n",
      "Chunk size: 2229 characters, 695 tokens\n",
      "Chunk size: 1037 characters, 188 tokens\n",
      "Chunk size: 1060 characters, 198 tokens\n",
      "Chunk size: 1032 characters, 199 tokens\n",
      "Chunk size: 1264 characters, 360 tokens\n",
      "Chunk size: 1016 characters, 194 tokens\n",
      "Chunk size: 1042 characters, 198 tokens\n",
      "Chunk size: 1010 characters, 184 tokens\n",
      "Chunk size: 1030 characters, 193 tokens\n",
      "Chunk size: 1565 characters, 392 tokens\n",
      "Chunk size: 1138 characters, 319 tokens\n",
      "Chunk size: 1264 characters, 311 tokens\n",
      "Chunk size: 1505 characters, 384 tokens\n",
      "Chunk size: 1084 characters, 195 tokens\n",
      "Chunk size: 1016 characters, 183 tokens\n",
      "Chunk size: 3358 characters, 897 tokens\n",
      "Chunk size: 1037 characters, 197 tokens\n",
      "Chunk size: 2880 characters, 753 tokens\n",
      "Chunk size: 1193 characters, 198 tokens\n",
      "Chunk size: 1067 characters, 197 tokens\n",
      "Chunk size: 1052 characters, 179 tokens\n",
      "Chunk size: 1071 characters, 252 tokens\n",
      "Chunk size: 1047 characters, 199 tokens\n",
      "Chunk size: 1030 characters, 187 tokens\n",
      "Chunk size: 1016 characters, 191 tokens\n",
      "Chunk size: 1716 characters, 430 tokens\n",
      "Chunk size: 1030 characters, 248 tokens\n",
      "Chunk size: 1075 characters, 291 tokens\n",
      "Chunk size: 1508 characters, 443 tokens\n",
      "Chunk size: 1379 characters, 351 tokens\n",
      "Chunk size: 1468 characters, 410 tokens\n",
      "Chunk size: 1469 characters, 392 tokens\n",
      "Chunk size: 1105 characters, 304 tokens\n",
      "Chunk size: 2785 characters, 719 tokens\n",
      "Chunk size: 1091 characters, 247 tokens\n",
      "Chunk size: 1235 characters, 364 tokens\n",
      "Chunk size: 1244 characters, 302 tokens\n",
      "Chunk size: 1059 characters, 198 tokens\n",
      "Chunk size: 1089 characters, 197 tokens\n",
      "Chunk size: 1046 characters, 189 tokens\n",
      "Chunk size: 1202 characters, 303 tokens\n",
      "Chunk size: 1272 characters, 292 tokens\n",
      "Chunk size: 1327 characters, 321 tokens\n",
      "Chunk size: 1120 characters, 247 tokens\n",
      "Chunk size: 3510 characters, 921 tokens\n",
      "Chunk size: 3961 characters, 946 tokens\n",
      "Chunk size: 2370 characters, 585 tokens\n",
      "Chunk size: 1047 characters, 189 tokens\n",
      "Chunk size: 2291 characters, 607 tokens\n",
      "Chunk size: 4352 characters, 1424 tokens\n"
     ]
    }
   ],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "for chunk in chunks:\n",
    "    if len(chunk) > 1000:\n",
    "        print(f\"Chunk size: {len(chunk)} characters, {len(tokenizer.encode(chunk))} tokens\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(768,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from nomic import embed\n",
    "import numpy as np\n",
    "\n",
    "output = embed.text(\n",
    "    texts=chunks,\n",
    "    model='nomic-embed-text-v1.5',\n",
    "    task_type='search_document',\n",
    ")\n",
    "\n",
    "embeddings = np.array(output['embeddings'])\n",
    "print(embeddings[0].shape)  # prints: (768,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = []\n",
    "for i, sentence in enumerate(chunks):\n",
    "    vectors.append({\n",
    "        \"id\": f\"txt{i}\",\n",
    "        \"values\": embeddings[i].tolist(),\n",
    "        \"metadata\": {\n",
    "            \"text\": sentence\n",
    "        }\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'IBA Institute of\\n Business Administr ation\\n Karachi\\nLeadership and Ideas for Tomorrow\\nPROGRA M ANNOU NCEMENT\\n2024-25\\n 2\\n V\\n---\\nPROGRAM\\n ANNOUNCEMENT 2024-25\\n---\\n PROGRAM 03 Table of Conter ts\\n ANNOUNCEMENT 2024-25\\n\\nMessage from the Executive Director 04 School of Economics & Social Sciences (SESS) 49\\nMessage from the Registrar 05 Programs on Offer 51\\nAcademic calendar 2024-2025 08 BS (Economics) 54\\nIBA in Numbers 10 BS (Economics and Mathematics) 59\\nAcademic programs 11 BS (Social Sciences and Liberal Arts) 64\\nDeans and Chairpersons 14 MS (Development Studies) 75\\nFee structure 15 MS (Economics) 77\\nFinancial assistance program 16 MS (Journalism) 79\\nFacilities at IBA 17 PhD (Economics) 80\\nStudent services 20\\nOfÔ¨Åce of Student Affairs 22 School of Mathematics & Computer Science (SMCS) 82\\nActivities studio 25 Programs on Offer 83\\n BS (Computer Science) 86\\nSchools BS (Mathematics) 92\\n MS (Computer Science) 95\\nSchool of Business Studies (SBS) 28 MS (Data Science) 97\\nPrograms on Offer 29 MS (Mathematics) 99\\nBBA 31 PhD (Computer Science) 101\\nBS (Accounting and Finance) 34 PhD (Mathematics) 102\\nMBA 38\\nEMBA 39 Student enrollment statistics 104\\nMS (Finance) 40 Calendar of Holidays 105\\nMS (Islamic Banking and Finance) 42 Excerpts of the academic calendar 2024-25 106\\nMS (Management) 44\\nMS (Marketing) 46\\n---\\n 04 PROGRAM\\n ANNOUNCEMENT 2024-25\\n\\nMessage from the\\nExecutive Director\\nIt gives me immense pleasure and pride to welcome you all to Pakistan‚Äôs leading\\ninstitution of higher learning, the IBA.'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors[1][\"metadata\"][\"text\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Upserted vectors: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 336/336 [00:06<00:00, 55.24it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "upserted_count: 336"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pinecone.grpc import PineconeGRPC as Pinecone\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "pc = Pinecone(api_key=os.getenv(\"PINECONE_API_KEY\"))\n",
    "# print(\"Pinecone Indices: \", pc.list_indexes())\n",
    "\n",
    "index = pc.Index(name=\"musab-bilal-rag\")\n",
    "index.describe_index_stats()\n",
    "\n",
    "index.upsert  ( \n",
    "    namespace=\"prog-ann\",\n",
    "    batch_size=32,\n",
    "    vectors=vectors\n",
    "    # vectors=embedding,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
